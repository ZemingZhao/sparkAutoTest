#!/bin/sh

##############################################
###    User Defined Cluster TOP Dir        ###
##############################################
export SPARK_HOME=/gpfs_fqi/zmzhao/spark-v1.5.2-hadoop2.6
export HADOOP_HOME=/opt/hadoop-2.7.1
export SYM_MASTER_HOST=red05
export TEST_TOOL_HOME=/gpfs_fqi/zmzhao/spark-v1.5.2-hadoop2.6/spark_autoTest_tool
export JAVA_HOME=/gpfs_fqi/zmzhao/sym71/jre/3.1/linux-x86_64

##############################################
###    Cluster Characters for Case Filter  ###
##############################################
export SLOTS_PER_HOST=12     #Slots of each compute host, specify for schedule and reclaim
export HOST_NUM=1            #Host number of ego cluster, specify for HA. default: 1
export DIST_FILE_SYSTEM=HDFS #Distrubuted file System in use. default: HDFS

##############################################
###    Env for Test, change if needed      ###
##############################################
export SUBMIT_USER=root      #OS User to submit spark workload
export SAMPLE_JAR=$TEST_TOOL_HOME/autoTestExamples.jar #out-of-box samples for test tool
export MASTER_LOG=$SPARK_HOME/logs/spark-$SUBMIT_USER-org.apache.spark.deploy.master.Master-1-$SYM_MASTER_HOST.out
export CASE_RUNNING_TIMEOUT=600    #each case will be killed if it keeps running beyond the timeout, in second
#export SPARK_CONF_DIR=$SPARK_HOME/conf 
